1. Tokenize Body Text - 35 mins @DT
------------------------------------
#Tokenize the 50m dump - 30 mins @DT
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=100 \
    -D mapred.reduce.tasks=1 \
    -libjars dig-lsh-clustering/karma-mr.jar \
    -file dig-lsh-clustering/tokenizer/body_text/mr_body_mapper.py \
    -mapper dig-lsh-clustering/tokenizer/body_text/mr_body_mapper.py \
    -file dig-lsh-clustering/tokenizer/body_text/mr_body_reducer.py \
    -reducer dig-lsh-clustering/tokenizer/body_text/mr_body_reducer.py \
    -input /user/worker/cleaner/2015-04-01/pilot/* \
    -output /user/ubuntu/dig-lsh/50m/tokens/1 \
    -inputformat edu.isi.karma.mapreduce.inputformat.SequenceFileAsJSONInputBatchFormat &

#Tokenize the incrementals - 4 mins @DT
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=100 \
    -D mapred.reduce.tasks=1 \
    -libjars dig-lsh-clustering/karma-mr.jar \
    -file dig-lsh-clustering/tokenizer/body_text/mr_body_mapper.py \
    -mapper dig-lsh-clustering/tokenizer/body_text/mr_body_mapper.py \
    -file dig-lsh-clustering/tokenizer/body_text/mr_body_reducer.py \
    -reducer dig-lsh-clustering/tokenizer/body_text/mr_body_reducer.py \
    -input /user/worker/cleaner/incremental/pilot/refactor/* \
    -output /user/ubuntu/dig-lsh/50m/tokens/2 \
    -inputformat edu.isi.karma.mapreduce.inputformat.SequenceFileAsJSONInputBatchFormat &

2. Generate LSH Keys - 1hrs, 38mins@DT
---------------------------------------
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=100 \
    -D mapred.reduce.tasks=100 \
    -file /home/ubuntu/dig-lsh-clustering/hasher/mr_str_lsh_mapper.py \
    -mapper /home/ubuntu/dig-lsh-clustering/hasher/mr_str_lsh_mapper.py \
    -file /home/ubuntu/dig-lsh-clustering/hasher/mr_lsh_reducer.py \
    -reducer /home/ubuntu/dig-lsh-clustering/hasher/mr_lsh_reducer.py \
    -input /user/ubuntu/dig-lsh/50m/tokens/* \
    -output /user/ubuntu/dig-lsh/50m/lsh &


3. Run the Clustering/SImilarity
----------------------------------
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=100 \
    -D mapred.reduce.tasks=100 \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_mapper.py \
    -mapper /home/ubuntu/dig-lsh-clustering/clusterer/cluster_mapper.py \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_reducer.py \
    -reducer /home/ubuntu/dig-lsh-clustering/clusterer/cluster_reducer.py \
    -input /user/ubuntu/dig-lsh/50m/lsh/* \
    -output /user/ubuntu/dig-lsh/50m/sim &

6. Remove the duplicates
------------------------
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=200 \
    -D mapred.reduce.tasks=100 \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_dedup_mapper.py \
    -mapper /home/ubuntu/dig-lsh-clustering/clusterer/cluster_dedup_mapper.py \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_dedup_reducer.py \
    -reducer /home/ubuntu/dig-lsh-clustering/clusterer/cluster_dedup_reducer.py \
    -input /user/ubuntu/dig-lsh/50m/sim/* \
    -output /user/ubuntu/dig-lsh/50m/sim-nodup &


7. Convert the output to JSON for the Hue Workflow
---------------------------------------------------
nohup time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.map.tasks=200 \
    -D mapred.reduce.tasks=100 \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_jsonoutput_mapper.py \
    -mapper /home/ubuntu/dig-lsh-clustering/clusterer/cluster_jsonoutput_mapper.py \
    -file /home/ubuntu/dig-lsh-clustering/clusterer/cluster_jsonoutput_reducer.py \
    -reducer /home/ubuntu/dig-lsh-clustering/clusterer/cluster_jsonoutput_reducer.py \
    -input /user/ubuntu/dig-lsh/50m/sim-nodup/* \
    -output /user/worker/ingest/istr50m/similar_body &


