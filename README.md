dig-lsh-clustering
==================

Clustering documents based on LSH

Requirements
------------
1. gensim: sudo pip install -U gensim
2. csvsort: sudo pip install csvsort

Usage
-----

Usage: 
```
runLSH.py --input <input filename> --output <output filename> [--separator <sep=\t>] [--dataType integer|string] [--numHashes <numHashes=20>] [--numItemsInBand <numItemsInBand=5>] [--minItemsInCluster <minItemsInCluster=2>]
```

* Input file is a tab separated file with key followed by tokens. The tokens can be separated by any separator that can be specified in the runLSH arguments
* The Output file is a JSON file with the clusters.

```gen_clusters``` contains sample clusters for dig dataset
  *  ```clean_title``` contains clusters generated using the deobfuscated title
      *  ```clean_title_preprocess.py``` takes the input json returned by the ES query and returns the file in a format required by runLSH
      * ```istr-100k-clean-title.json``` is the json retuned by ES query
      * ```istr-100k-clean-title-for-lsh.tsv``` is the file generated by the ```clean_title_preprocess.py``` script for inputing to runLSH
      * ```istr-100k-clean-title-clusters.json``` are the clusters generated by runLSH using the following command:
        
        ```
         python runLSH.py --input gen_clusters/clean_title/istr-100k-clean-title-for-lsh.tsv --output gen_clusters/clean_title/istr-100k-clean-title-clusters.json --dataType string --numItemsInBand 10 --numHashes 20
        ```

  ```gen_int_input``` contains code to take in a tab separated file of Key\ttokens and convert the string tokens to integers and output the string\tinteger hashmap.
  *  ```clean_title``` contains clusters generated using the deobfuscated title
      *  ```clean_title_preprocess.py``` takes the input json returned by the ES query and returns the file in a format required by runLSH
      * ```istr-100k-clean-title.json``` is the json retuned by ES query
      * ```istr-100k-clean-title-for-lsh.tsv``` is the file generated by the ```clean_title_preprocess.py``` script for inputing to runLSH
      * ```istr-100k-clean-title-clusters.json``` are the clusters generated by runLSH using the following command:

        ```
         python gen_int_input/str_to_nt_tokens.py  gen_clusters/clean_title/istr-100k-clean-title-for-lsh.tsv  gen_clusters/clean_title/istr-100k-clean-title-for-lsh-int.tsv gen_clusters/clean_title/istr-100k-clean-title-for-lsh-hashmap.tsv
        ```

        To generate the clusters now with integer input, run:
        ```
         python runLSH.py --input gen_clusters/clean_title/istr-100k-clean-title-for-lsh-int.tsv --output gen_clusters/clean_title/istr-100k-clean-title-clusters-int.json --dataType integer --numItemsInBand 10 --numHashes 20
        ```

v2: LSH using Spark
===================

Requirements:
-------------
Download and unzip spark in <spark-folder>

You can run the clustering using a One Step driver - runLSH.py or using 3 steps - tokenization, hashing and then clustering

Tokenization, LSH, Clustering using one step
--------------------------------------------
```
runLSH.py [options] inputFile configFile outputDir
```

To view all options, you can pass --help to the programs. Example:
```
./bin/spark-submit  ~/github/dig-lsh-clustering/runLSH.py --help
```

Example Invocation:
```
 zip -r lsh.zip tokenizer hasher clusterer
./bin/spark-submit \
    --master local[*] \
    --py-files ~/github/dig-lsh-clustering/lsh.zip \
    ~/github/dig-lsh-clustering/runLSH.py \
    --base ~/github/dig-lsh-clustering/datasets/geonames/sample.tsv \
    --numHashes 50 --numItemsInBand 5 \
    --computeSimilarity \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/sample.tsv \
    ~/github/dig-lsh-clustering/datasets/city_state_country_config.json \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/geonames-clusters
```
The output is in text file format. If you wish to generate the output as
a text file, pass ```--outputformat sequence``` as a parameter


Running tokenization, LSH, clustering Step-by-Step
--------------------------------------------------
Step 1: Tokenization
---------------------
```
tokenizer.py [options] inputFile configFile outputDir
```

Example Invocation:
```
cd tokenizer
zip -r tokenizer.zip RowTokenizer.py inputParser
cd <spark-folder>
./bin/spark-submit \
    --master local[*] \
    --py-files ~/github/dig-lsh-clustering/tokenizer/tokenizer.zip \
    ~/github/dig-lsh-clustering/tokenizer/tokenizer.py \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/sample.tsv \
    ~/github/dig-lsh-clustering/datasets/city_state_country_config.json \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/tokens

./bin/spark-submit \
    --master local[*] \
    --py-files ~/github/dig-lsh-clustering/tokenizer/tokenizer.zip \
    ~/github/dig-lsh-clustering/tokenizer/tokenizer.py \
    ~/github/dig-lsh-clustering/datasets/geonames/sample.tsv \
    ~/github/dig-lsh-clustering/datasets/city_state_country_config.json \
    ~/github/dig-lsh-clustering/datasets/geonames/tokens
```

Step 2: Compute LSH
---------------------
```
hasher.py [options] inputDir outputDir
```

Example Invocation:
```
cd hasher
zip -r hasher.zip lsh
cd <spark-folder>
./bin/spark-submit \
    --master local[*] \
    --py-files ~/github/dig-lsh-clustering/hasher/hasher.zip \
    ~/github/dig-lsh-clustering/hasher/hasher.py \
    --saveMinhashes --numHashes 50 --numItemsInBand 5 \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/tokens \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/hashes

./bin/spark-submit \
    --master local[*] \
    --py-files ~/github/dig-lsh-clustering/hasher/hasher.zip \
    ~/github/dig-lsh-clustering/hasher/hasher.py \
    --saveMinhashes --numHashes 50 --numItemsInBand 5 \
    ~/github/dig-lsh-clustering/datasets/geonames/tokens \
    ~/github/dig-lsh-clustering/datasets/geonames/hashes
```
You can omit the --saveMinhashes parameter if you do not want a similarity score

Step 3: Perform the clustering
------------------------------
```
clusterer.py [options] inputDir outputDir
```

Example Invocation:
```
./bin/spark-submit \
     --master local[*] \
    ~/github/dig-lsh-clustering/clusterer/clusterer.py \
    --base ~/github/dig-lsh-clustering/datasets/geonames/hashes \
    --computeSimilarity \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/hashes \
    ~/github/dig-lsh-clustering/datasets/sample-ad-location/geonames-clusters
```
